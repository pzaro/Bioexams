import streamlit as st
from google.cloud import vision
from google.oauth2 import service_account
from pdf2image import convert_from_bytes
import pandas as pd
import io
import re
import scipy.stats as stats
from fpdf import FPDF

# =========================
# 1) SETUP
# =========================
st.set_page_config(page_title="Medical Lab Commander", layout="wide")

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');
html, body, .stDataFrame { font-family: 'Roboto', sans-serif; }
.stDataFrame td, .stDataFrame th { text-align: center !important; vertical-align: middle !important; }
.stDataFrame th { background-color: #ff4b4b !important; color: white !important; }
h1, h2, h3 { text-align: center; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ©¸ Medical Lab Commander")
st.markdown("<h5 style='text-align: center;'>V16: Strict Only + Print PDF + PLT Default</h5>", unsafe_allow_html=True)

# =========================
# 2) AUTH
# =========================
def get_vision_client():
    try:
        key_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(key_dict)
        return vision.ImageAnnotatorClient(credentials=creds)
    except Exception as e:
        st.error(f"Auth Error: {e}")
        return None

# =========================
# 3) HELPERS
# =========================
def normalize_line(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s

def clean_number(val_str: str):
    if not val_str:
        return None

    s = val_str.strip()
    s = s.replace('"', '').replace("'", "").replace(':', '')
    s = s.replace('*', '').replace('$', '').replace('â‰¤', '').replace('â‰¥', '')
    s = s.replace('<', '').replace('>', '')
    s = s.replace('O', '0').replace('o', '0')
    s = s.replace('â€“', '-').replace('âˆ’', '-')
    s = re.sub(r"[^0-9,.\-]", "", s)

    if "," in s and "." in s:
        last_comma = s.rfind(",")
        last_dot = s.rfind(".")
        if last_comma > last_dot:
            s = s.replace(".", "")
            s = s.replace(",", ".")
        else:
            s = s.replace(",", "")
    else:
        if "," in s and "." not in s:
            s = s.replace(".", "")
            s = s.replace(",", ".")

    s = s.strip()
    if s.count("-") > 1:
        s = s.replace("-", "")
    if "-" in s and not s.startswith("-"):
        s = s.replace("-", "")

    try:
        return float(s)
    except:
        return None

def find_all_numbers(s: str):
    if not s:
        return []
    s_clean = s.replace('"', ' ').replace("'", " ").replace(':', ' ')
    candidates = re.findall(
        r"[-]?\d{1,3}(?:[.,]\d{3})*(?:[.,]\d+)?|[-]?\d+(?:[.,]\d+)?",
        s_clean
    )
    out = []
    for c in candidates:
        v = clean_number(c)
        if v is not None:
            out.append(v)
    return out

def keyword_hit(line_upper: str, kw: str) -> bool:
    kw = (kw or "").upper().strip()
    if not kw:
        return False

    if " " in kw:
        return kw in line_upper

    # Î Î¹Î¬Î½ÎµÎ¹ R B C / R.B.C / R-B-C Îº.Î»Ï€.
    if 2 <= len(kw) <= 5 and re.fullmatch(r"[A-Z0-9]+", kw):
        spaced = r"\W*".join(list(map(re.escape, kw)))
        if re.search(spaced, line_upper):
            return True

    pattern = r"(?:^|[^A-Z0-9Î‘-Î©])" + re.escape(kw) + r"(?:$|[^A-Z0-9Î‘-Î©])"
    return re.search(pattern, line_upper) is not None

def pick_best_value(metric_name: str, values: list[float]):
    m = (metric_name or "").upper()
    values = [v for v in values if v is not None]
    if not values:
        return None

    if "WBC" in m or "Î›Î•Î¥Îš" in m:
        vals = [v for v in values if 0.1 <= v <= 30]
        return vals[0] if vals else None

    if "RBC" in m or "Î•Î¡Î¥Î˜" in m:
        vals = [v for v in values if 1.0 <= v <= 8.0]
        return vals[0] if vals else None

    if "HGB" in m or "Î‘Î™ÎœÎŸÎ£Î¦" in m:
        vals = [v for v in values if 5.0 <= v <= 25.0]
        return vals[0] if vals else None

    if "HCT" in m or "Î‘Î™ÎœÎ‘Î¤ÎŸÎš" in m:
        vals = [v for v in values if 10.0 <= v <= 70.0]
        return vals[0] if vals else None

    if "PLT" in m or "Î‘Î™ÎœÎŸÎ Î•Î¤" in m or "PLATE" in m:
        vals = [v for v in values if 10 <= v <= 2000]
        ints = [v for v in vals if abs(v - round(v)) < 1e-6]
        return ints[0] if ints else (vals[0] if vals else None)

    # default
    return values[0]

def parse_google_text_deep(full_text: str, selected_metrics: dict, debug: bool = False):
    results = {}
    debug_rows = []

    lines = [normalize_line(x) for x in (full_text or "").split("\n")]
    lines = [x for x in lines if x]

    all_possible_keywords = set()
    for k_list in selected_metrics.values():
        for k in k_list:
            if k:
                all_possible_keywords.add(k.upper().strip())

    for metric_name, keywords in selected_metrics.items():
        current_keywords = [k.upper().strip() for k in keywords if k]

        found_at_line = ""
        candidates = []
        picked = None

        for i, line in enumerate(lines):
            line_upper = line.upper()

            if any(keyword_hit(line_upper, k) for k in current_keywords):
                found_at_line = line
                candidates = []
                candidates += find_all_numbers(line)

                max_lookahead = 10 if "RBC" in metric_name.upper() else 7

                for offset in range(1, max_lookahead):
                    if i + offset >= len(lines):
                        break

                    nxt = lines[i + offset]
                    nxt_upper = nxt.upper()

                    # STOP if next line belongs to another test
                    found_other = False
                    for known_k in all_possible_keywords:
                        if known_k not in current_keywords and keyword_hit(nxt_upper, known_k):
                            found_other = True
                            break
                    if found_other:
                        break

                    candidates += find_all_numbers(nxt)

                picked = pick_best_value(metric_name, candidates)

                # Block years, except B12
                if picked is not None and (1990 < picked < 2030) and ("B12" not in metric_name.upper()):
                    picked = None

                if picked is not None:
                    results[metric_name] = picked
                break

        if debug:
            debug_rows.append({
                "Metric": metric_name,
                "MatchedLine": found_at_line,
                "Candidates": ", ".join([str(x) for x in candidates]),
                "Picked": results.get(metric_name, None)
            })

    dbg_df = pd.DataFrame(debug_rows) if debug else None
    return results, dbg_df

# =========================
# 4) OCR
# =========================
def ocr_pdf_to_text(client, pdf_bytes: bytes, dpi: int = 300) -> str:
    images = convert_from_bytes(pdf_bytes, dpi=dpi, fmt="png", grayscale=True)

    full_text = ""
    for img in images:
        buf = io.BytesIO()
        img.save(buf, format="PNG", optimize=True)
        content = buf.getvalue()

        image = vision.Image(content=content)
        response = client.document_text_detection(image=image)

        if response.error.message:
            st.warning(f"OCR warning: {response.error.message}")

        if response.full_text_annotation and response.full_text_annotation.text:
            full_text += response.full_text_annotation.text + "\n"
        elif response.text_annotations:
            full_text += response.text_annotations[0].description + "\n"

    return full_text

def extract_date_from_text_or_filename(full_text: str, filename: str):
    date_match = re.search(r'(\d{1,2}/\d{1,2}/\d{2,4})', full_text or "")
    if date_match:
        return pd.to_datetime(date_match.group(1), dayfirst=True, errors='coerce')

    m = re.search(r'(\d{6})', filename or "")
    if m:
        d_str = m.group(1)
        return pd.to_datetime(f"{d_str[4:6]}/{d_str[2:4]}/20{d_str[0:2]}", dayfirst=True, errors='coerce')

    return pd.NaT

# =========================
# 5) PDF (Print-ready)
# =========================
def create_print_pdf(display_df: pd.DataFrame, title: str = "Medical Lab Report"):
    pdf = FPDF(orientation="P", unit="mm", format="A4")
    pdf.add_page()
    pdf.set_auto_page_break(auto=True, margin=12)

    pdf.set_font("Arial", "B", 16)
    pdf.cell(0, 10, title, ln=True, align="C")
    pdf.ln(2)

    # Table settings
    pdf.set_font("Arial", "B", 9)

    cols = list(display_df.columns)

    # Dynamic column widths (basic, practical)
    # Date Î¼Î¹ÎºÏÏŒ, Î‘ÏÏ‡ÎµÎ¯Î¿ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿, Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î± Î¼ÎµÏƒÎ±Î¯Î±
    col_widths = []
    for c in cols:
        if c.lower() == "date":
            col_widths.append(25)
        elif c == "Î‘ÏÏ‡ÎµÎ¯Î¿":
            col_widths.append(70)
        else:
            col_widths.append(30)

    # Header
    for c, w in zip(cols, col_widths):
        pdf.cell(w, 8, str(c)[:20], border=1, align="C")
    pdf.ln()

    pdf.set_font("Arial", "", 9)
    for _, row in display_df.iterrows():
        for c, w in zip(cols, col_widths):
            val = "" if pd.isna(row[c]) else str(row[c])
            pdf.cell(w, 8, val[:35], border=1, align="C")
        pdf.ln()

    return pdf.output(dest="S").encode("latin-1", "ignore")

# =========================
# 6) STATS + EXPLANATION TEXT
# =========================
def stats_method_explanation(method: str = "pearson"):
    if method == "pearson":
        return """
**ÎœÎ­Î¸Î¿Î´Î¿Ï‚ ÏƒÏ…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ·Ï‚: Pearson correlation (r)**

Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ ÏŒÏ„Î±Î½ ÏƒÏ…Î³ÎºÏÎ¯Î½Î¿Ï…Î¼Îµ **Î´ÏÎ¿ ÏƒÏ…Î½ÎµÏ‡ÎµÎ¯Ï‚ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ­Ï‚ Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚** ÎºÎ±Î¹ Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± Î¼ÎµÏ„ÏÎ®ÏƒÎ¿Ï…Î¼Îµ Ï„Î·Î½ **Î³ÏÎ±Î¼Î¼Î¹ÎºÎ®** Ï„Î¿Ï…Ï‚ ÏƒÏ‡Î­ÏƒÎ·.
- r âˆˆ [-1, +1]
- r > 0: Î¸ÎµÏ„Î¹ÎºÎ® Î³ÏÎ±Î¼Î¼Î¹ÎºÎ® ÏƒÏ…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ·
- r < 0: Î±ÏÎ½Î·Ï„Î¹ÎºÎ® Î³ÏÎ±Î¼Î¼Î¹ÎºÎ® ÏƒÏ…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ·
- r â‰ˆ 0: Î±Ï€Î¿Ï…ÏƒÎ¯Î± Î³ÏÎ±Î¼Î¼Î¹ÎºÎ®Ï‚ ÏƒÏ…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ·Ï‚

**ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚ (p-value):**
- H0: Ï = 0 (ÎºÎ±Î¼Î¯Î± ÏƒÏ…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ· ÏƒÏ„Î¿Î½ Ï€Î»Î·Î¸Ï…ÏƒÎ¼ÏŒ)
- Î‘Î½ p < 0.05: Î­Î½Î´ÎµÎ¹Î¾Î· ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ®Ï‚ ÏƒÏ…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ·Ï‚

**Î“Î¹Î±Ï„Î¯ ÎµÏ€Î¹Î»Î­Ï‡Î¸Î·ÎºÎµ ÎµÎ´Ï:**
Î¤Î± ÎµÏÎ³Î±ÏƒÏ„Î·ÏÎ¹Î±ÎºÎ¬ Î¼ÎµÎ³Î­Î¸Î· (PLT, WBC Îº.Î»Ï€.) ÎµÎ¯Î½Î±Î¹ ÏƒÏ…Î½ÎµÏ‡ÎµÎ¯Ï‚ Ï„Î¹Î¼Î­Ï‚ ÎºÎ±Î¹ Î· Î²Î±ÏƒÎ¹ÎºÎ® ÎµÏÏÏ„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î±Î½ ÏƒÏ…Î½-Î¼ÎµÏ„Î±Î²Î¬Î»Î»Î¿Î½Ï„Î±Î¹ Î³ÏÎ±Î¼Î¼Î¹ÎºÎ¬.

**Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯:**
- Î•Ï…Î±Î¹ÏƒÎ¸Î·ÏƒÎ¯Î± ÏƒÎµ outliers
- ÎœÎµ Ï€Î¿Î»Ï Î¼Î¹ÎºÏÏŒ N, Î· p-value Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î±Î¸ÎµÏÎ®/Î¹ÏƒÏ‡Ï…ÏÎ®
- Î‘Î½ Î· ÏƒÏ‡Î­ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î¼Î· Î³ÏÎ±Î¼Î¼Î¹ÎºÎ® Î±Î»Î»Î¬ Î¼Î¿Î½Î¿Ï„Î¿Î½Î¹ÎºÎ®, Î¿ Pearson Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± â€œÏ…Ï€Î¿ÎµÎºÏ„Î¹Î¼Î®ÏƒÎµÎ¹â€ Ï„Î· ÏƒÏ‡Î­ÏƒÎ·

**Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ® (ÏŒÏ„Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹):**
Spearman rho (rank-based) Î³Î¹Î± Î¼Î·-ÎºÎ±Î½Î¿Î½Î¹ÎºÏŒÏ„Î·Ï„Î±/outliers Î® Î¼Î¿Î½Î¿Ï„Î¿Î½Î¹ÎºÎ® ÏƒÏ‡Î­ÏƒÎ·.
"""

def run_statistics_pearson(df, col_x, col_y):
    clean_df = df[[col_x, col_y]].apply(pd.to_numeric, errors='coerce').dropna()
    if len(clean_df) < 3:
        return f"âš ï¸ Î§ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ 3+ Î¼ÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚ (Î²ÏÎ­Î¸Î·ÎºÎ±Î½ {len(clean_df)}).", None

    x = clean_df[col_x]
    y = clean_df[col_y]
    if x.std() == 0 or y.std() == 0:
        return "âš ï¸ Î£Ï„Î±Î¸ÎµÏÎ® Ï„Î¹Î¼Î® ÏƒÎµ Î¼Î¯Î± Î±Ï€ÏŒ Ï„Î¹Ï‚ Î´ÏÎ¿ Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚ (Î¼Î·Î´ÎµÎ½Î¹ÎºÎ® Î´Î¹Î±ÎºÏÎ¼Î±Î½ÏƒÎ·).", None

    corr, p_value = stats.pearsonr(x, y)
    return {
        "N": len(clean_df),
        "Pearson r": corr,
        "p-value": p_value
    }, clean_df

# =========================
# 7) METRICS DB
# =========================
ALL_METRICS_DB = {
    "PLT (Î‘Î¹Î¼Î¿Ï€ÎµÏ„Î¬Î»Î¹Î±)": ["PLT", "Platelets", "Î‘Î¹Î¼Î¿Ï€ÎµÏ„Î¬Î»Î¹Î±"],
    "WBC (Î›ÎµÏ…ÎºÎ¬)": ["WBC", "Î›ÎµÏ…ÎºÎ¬"],
    "RBC (Î•ÏÏ…Î¸ÏÎ¬)": ["RBC", "R.B.C", "ERY", "ER", "Î•ÏÏ…Î¸ÏÎ¬", "Î•ÏÏ…Î¸ÏÎ¿ÎºÏÏ„Ï„Î±ÏÎ±", "Î•ÏÏ…Î¸ÏÎ¿ÎºÏ…Ï„Ï„Î±Ï"],
    "HGB (Î‘Î¹Î¼Î¿ÏƒÏ†Î±Î¹ÏÎ¯Î½Î·)": ["HGB", "H.B.G", "Î‘Î¹Î¼Î¿ÏƒÏ†Î±Î¹ÏÎ¯Î½Î·"],
    "HCT (Î‘Î¹Î¼Î±Ï„Î¿ÎºÏÎ¯Ï„Î·Ï‚)": ["HCT", "Î‘Î¹Î¼Î±Ï„Î¿ÎºÏÎ¯Ï„Î·Ï‚"],
    "MCV": ["MCV"],
    "MCH": ["MCH"],
    "MCHC": ["MCHC"],
    "RDW": ["RDW"],
    "MPV": ["MPV"],
    "PCT": ["PCT"],
    "PDW": ["PDW"],
}

# =========================
# 8) SESSION STATE
# =========================
if "df_master" not in st.session_state:
    st.session_state.df_master = None
if "debug_master" not in st.session_state:
    st.session_state.debug_master = None

# =========================
# 9) SIDEBAR
# =========================
st.sidebar.header("âš™ï¸ Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
uploaded_files = st.sidebar.file_uploader("Î‘Î½Î­Î²Î±ÏƒÎµ PDF", type="pdf", accept_multiple_files=True)

dpi = st.sidebar.slider("Î Î¿Î¹ÏŒÏ„Î·Ï„Î± OCR (DPI)", 200, 400, 300, 50)
show_debug = st.sidebar.checkbox("Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Debug", value=False)

all_keys = list(ALL_METRICS_DB.keys())

# Î ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î® ÎœÎŸÎÎŸ PLT
default_selected = ["PLT (Î‘Î¹Î¼Î¿Ï€ÎµÏ„Î¬Î»Î¹Î±)"]

selected_metric_keys = st.sidebar.multiselect(
    "Î•Î¾ÎµÏ„Î¬ÏƒÎµÎ¹Ï‚:",
    all_keys,
    default=default_selected
)

active_metrics_map = {k: ALL_METRICS_DB[k] for k in selected_metric_keys}

if st.sidebar.button("ğŸš€ START") and uploaded_files:
    client = get_vision_client()
    if not client:
        st.stop()

    all_data = []
    debug_tables = []

    bar = st.progress(0.0)

    for i, file in enumerate(uploaded_files):
        try:
            pdf_bytes = file.getvalue()
            full_text = ocr_pdf_to_text(client, pdf_bytes, dpi=dpi)

            data, dbg = parse_google_text_deep(full_text, active_metrics_map, debug=show_debug)

            the_date = extract_date_from_text_or_filename(full_text, file.name)
            data["Date"] = the_date
            data["Î‘ÏÏ‡ÎµÎ¯Î¿"] = file.name
            all_data.append(data)

            if show_debug and dbg is not None:
                dbg["Date"] = the_date
                dbg["Î‘ÏÏ‡ÎµÎ¯Î¿"] = file.name
                debug_tables.append(dbg)

        except Exception as e:
            st.error(f"Error {file.name}: {e}")

        bar.progress((i + 1) / len(uploaded_files))

    if all_data:
        st.session_state.df_master = pd.DataFrame(all_data).sort_values("Date")
        st.success("Done!")
    else:
        st.warning("Î”ÎµÎ½ ÎµÎ¾Î®Ï‡Î¸Î·ÏƒÎ±Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î±.")

    if show_debug and debug_tables:
        st.session_state.debug_master = pd.concat(debug_tables, ignore_index=True)
    else:
        st.session_state.debug_master = None

# =========================
# 10) DASHBOARD
# =========================
if st.session_state.df_master is not None:
    df = st.session_state.df_master.copy()

    cols = ["Date", "Î‘ÏÏ‡ÎµÎ¯Î¿"] + [c for c in selected_metric_keys if c in df.columns]
    final_df = df[cols].copy()

    display_df = final_df.copy()
    display_df["Date"] = pd.to_datetime(display_df["Date"], errors="coerce").dt.strftime("%d/%m/%Y")

    st.subheader("ğŸ“‹ Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±")
    st.dataframe(display_df, use_container_width=True)

    # Debug table (Î±Î½ Ï„Î¿ Î­Ï‡ÎµÎ¹Ï‚ ÎµÎ½ÎµÏÎ³ÏŒ)
    if st.session_state.debug_master is not None:
        st.subheader("ğŸ§ª Debug (Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ· ÎµÎ¾Î±Î³Ï‰Î³Î®Ï‚)")
        dbg_show = st.session_state.debug_master.copy()
        dbg_show["Date"] = pd.to_datetime(dbg_show["Date"], errors="coerce").dt.strftime("%d/%m/%Y")
        st.dataframe(dbg_show[["Date", "Î‘ÏÏ‡ÎµÎ¯Î¿", "Metric", "MatchedLine", "Candidates", "Picked"]], use_container_width=True)

    st.divider()

    # PDF PRINT BUTTON
    st.subheader("ğŸ–¨ï¸ Î•ÎºÏ„ÏÏ€Ï‰ÏƒÎ·")
    pdf_bytes = create_print_pdf(display_df, title="Medical Lab Report (Print)")
    st.download_button(
        "ğŸ“„ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± PDF Î³Î¹Î± Î•ÎºÏ„ÏÏ€Ï‰ÏƒÎ·",
        data=pdf_bytes,
        file_name="medical_lab_print.pdf",
        mime="application/pdf"
    )

    st.divider()

    # Stats section
    st.subheader("ğŸ§® Î£Ï…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ· / Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ®")
    stat_cols = [c for c in cols if c not in ["Date", "Î‘ÏÏ‡ÎµÎ¯Î¿"]]
    if len(stat_cols) >= 2:
        c1, c2 = st.columns(2)
        with c1:
            x_ax = st.selectbox("X (Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î®)", stat_cols, index=0)
        with c2:
            y_ax = st.selectbox("Y (Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î®)", stat_cols, index=1)

        if st.button("Run Correlation"):
            if x_ax == y_ax:
                st.warning("Î”Î¹Î¬Î»ÎµÎ¾Îµ Î´ÏÎ¿ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚.")
            else:
                st.markdown(stats_method_explanation("pearson"))
                res, clean_df = run_statistics_pearson(final_df, x_ax, y_ax)
                if clean_df is None:
                    st.warning(res)
                else:
                    st.write({
                        "N": res["N"],
                        "Pearson r": round(res["Pearson r"], 4),
                        "p-value": round(res["p-value"], 6),
                    })
    else:
        st.info("Î“Î¹Î± ÏƒÏ…ÏƒÏ‡Î­Ï„Î¹ÏƒÎ· Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 2 ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½ÎµÏ‚ ÎµÎ¾ÎµÏ„Î¬ÏƒÎµÎ¹Ï‚.")
